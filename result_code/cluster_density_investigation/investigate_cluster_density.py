#!/usr/bin/env python3
"""
Cluster Density Investigation Script

This script analyzes cluster densities (mRNA/µm³) across many FOVs to identify
potential mis-clustering issues. Low density clusters may indicate:
- Over-segmentation (splitting clusters that should be one)
- False positive cluster detection
- Poor image quality regions

PREREQUISITES:
- Run fig_aggregate_scaling_v3.py first to generate peak_intensities_per_slide.csv
- Run fig_negative_threshold.py first to generate photon_thresholds.csv

Features:
1. Analyze X FOVs from randomly selected animals
2. Store cluster data in a DataFrame for distribution analysis
3. Save cluster images with density-sortable filenames (for napari/file browser viewing)
4. Identify FOVs with many low-density clusters
5. Enable debugging by viewing problematic FOVs and clusters

Usage:
    python investigate_cluster_density.py --n_fovs 100 --output_dir ./output
    python investigate_cluster_density.py --n_fovs 100 --save_images  # Also save cluster images

Output:
    output/
        cluster_data.csv           # All cluster data with densities
        fov_summary.csv            # Per-FOV statistics
        density_distributions.pdf   # Distribution plots
        cluster_images/            # (with --save_images) All cluster thumbnails
            0025.30_green_m1a1_r003_0042.png   # Sorted by density!
            0031.45_orange_m2b4_r015_0017.png
            ...

Cluster image filenames are formatted as:
    {density:07.2f}_{channel}_{slide}_{region}_{label_id}.png

This allows sorting by density in any file browser or loading into napari.
"""

import os
import sys
import time
from pathlib import Path
import numpy as np
import pandas as pd
import h5py
from tifffile import imwrite
from skimage import measure
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
from PIL import Image, ImageDraw
import blosc
import traceback
import torch
import argparse
import random

# Add parent directories for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'final_figures'))
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'user_code'))

# Import RNAscope class
from rna_scope_backbone.RNAscope import RNAscopeclass

# Import from results_config
sys.path.insert(0, str(Path(__file__).parent.parent))
from results_config import (
    CHANNEL_PARAMS, SLICE_DEPTH, PIXELSIZE, VOXEL_SIZE, SIGMA_Z_XLIM,
    H5_FILE_PATH_EXPERIMENTAL, EXCLUDED_SLIDES
)

# ══════════════════════════════════════════════════════════════════════════
# CONSTANTS
# ══════════════════════════════════════════════════════════════════════════

# Voxel volume for density calculation
VOXEL_VOLUME_UM3 = VOXEL_SIZE  # µm³ per voxel

# Pixel size for spatial calculations
PIXEL_SIZE_XY = PIXELSIZE / 1000  # µm per pixel
SLICE_DEPTH_UM = SLICE_DEPTH / 1000  # µm per z-slice

# Channel indices in NPZ files
CHANNEL_MAP = {
    'blue': 0,
    'green': 1,
    'orange': 2,
    'red': 3,
}

# Local model paths
LOCAL_MODEL_PATHS = {
    'green': '/home/grunwaldlab/development/rna_scope/training_cluster/training_green_yellow/checkpoint/best_checkpoint.pytorch',
    'orange': '/home/grunwaldlab/development/rna_scope/training_cluster/training_green_yellow/checkpoint/best_checkpoint.pytorch',
}

# Local YAML config paths
LOCAL_YAML_PATHS = {
    'green': '/home/grunwaldlab/development/rna_scope/predict_yamls/confg_prediction_green_yellowchannel.yaml',
    'orange': '/home/grunwaldlab/development/rna_scope/predict_yamls/confg_prediction_green_yellowchannel.yaml',
}

# Path to pre-computed peak intensities (from fig_aggregate_scaling_v3.py)
PEAK_INTENSITIES_CSV = Path(__file__).parent.parent / 'draft_figures' / 'output' / 'aggregate_scaling' / 'peak_intensities_per_slide.csv'

# Path to photon thresholds CSV (from fig_negative_threshold.py)
PHOTON_THRESHOLDS_CSV = Path(__file__).parent.parent / 'final_figures' / 'output' / 'photon_thresholds.csv'


# ══════════════════════════════════════════════════════════════════════════
# HELPER FUNCTIONS
# ══════════════════════════════════════════════════════════════════════════

def extract_slide_from_fov_key(fov_key: str) -> str:
    """Extract slide name from FOV key (e.g., 'q111...--m3a1--...' -> 'm3a1')."""
    import re
    match = re.search(r'--([m]\d+[ab]\d+)--', fov_key.lower())
    if match:
        return match.group(1)
    return None


def load_slide_peak_intensities() -> dict:
    """
    Load pre-computed slide-specific peak intensities from CSV.
    This CSV is generated by fig_aggregate_scaling_v3.py and MUST exist.

    Returns dict: {(slide, channel): peak_intensity_photons}
    """
    if not PEAK_INTENSITIES_CSV.exists():
        raise FileNotFoundError(
            f"Peak intensities CSV not found at {PEAK_INTENSITIES_CSV}\n"
            f"Please run fig_aggregate_scaling_v3.py first to generate this file."
        )

    df = pd.read_csv(PEAK_INTENSITIES_CSV)
    peak_dict = {}
    for _, row in df.iterrows():
        slide = row['slide']
        channel = row['channel']
        peak_intensity = row['peak_intensity_photons']
        peak_dict[(slide, channel)] = peak_intensity

    print(f"Loaded {len(peak_dict)} slide-specific peak intensities from CSV")
    return peak_dict


# Global cache for peak intensities
_SLIDE_PEAK_INTENSITIES = None


def get_slide_peak_intensity(slide: str, channel: str) -> float:
    """
    Get pre-computed peak intensity for a slide/channel combination.
    Raises an error if not found (no fallback - data must exist).
    """
    global _SLIDE_PEAK_INTENSITIES
    if _SLIDE_PEAK_INTENSITIES is None:
        _SLIDE_PEAK_INTENSITIES = load_slide_peak_intensities()

    key = (slide, channel)
    if key not in _SLIDE_PEAK_INTENSITIES:
        raise ValueError(f"No peak intensity found for slide={slide}, channel={channel}")

    return _SLIDE_PEAK_INTENSITIES[key]


def load_photon_thresholds() -> dict:
    """
    Load negative control thresholds from CSV.
    This CSV is generated by fig_negative_threshold.py and MUST exist.

    Returns dict: {(slide, channel): threshold}
    """
    if not PHOTON_THRESHOLDS_CSV.exists():
        raise FileNotFoundError(
            f"Photon thresholds CSV not found at {PHOTON_THRESHOLDS_CSV}\n"
            f"Please run fig_negative_threshold.py first to generate this file."
        )

    df = pd.read_csv(PHOTON_THRESHOLDS_CSV)
    thresh_dict = {}
    import ast
    for _, row in df.iterrows():
        key_str = row['key']
        key_tuple = ast.literal_eval(key_str)
        slide = key_tuple[0]
        channel = key_tuple[1]
        thresh_dict[(slide, channel)] = row['threshold']

    print(f"Loaded {len(thresh_dict)} slide-specific photon thresholds from CSV")
    return thresh_dict


# Global cache
_PHOTON_THRESHOLDS = None


def get_photon_threshold(slide: str, channel: str) -> float:
    """Get negative control threshold for a slide/channel combination."""
    global _PHOTON_THRESHOLDS
    if _PHOTON_THRESHOLDS is None:
        _PHOTON_THRESHOLDS = load_photon_thresholds()

    key = (slide, channel)
    if key not in _PHOTON_THRESHOLDS:
        raise ValueError(f"No photon threshold found for slide={slide}, channel={channel}")

    return _PHOTON_THRESHOLDS[key]


def compute_cluster_variance_stats(label_mask: np.ndarray, image: np.ndarray, label_id: int) -> dict:
    """
    Compute variance statistics for a single cluster.

    Args:
        label_mask: 3D label array where each cluster has unique integer ID
        image: 3D intensity image (photon counts)
        label_id: The label ID to compute stats for

    Returns:
        dict with {mean_photons, std_photons, cv_photons, iqr_photons, pct_range_photons, max_photons}
    """
    mask = label_mask == label_id
    voxel_values = image[mask]

    if len(voxel_values) == 0:
        return {
            'mean_photons': 0.0,
            'std_photons': 0.0,
            'cv_photons': 0.0,
            'iqr_photons': 0.0,
            'pct_range_photons': 0.0,
            'max_photons': 0.0,
        }

    mean_val = np.mean(voxel_values)
    std_val = np.std(voxel_values)
    cv = std_val / mean_val if mean_val > 0 else 0

    # Percentile-based metrics
    p5, p25, p75, p95 = np.percentile(voxel_values, [5, 25, 75, 95])
    iqr = p75 - p25
    pct_range = p95 - p5

    # Max intensity within cluster
    max_val = np.max(voxel_values)

    return {
        'mean_photons': float(mean_val),
        'std_photons': float(std_val),
        'cv_photons': float(cv),
        'iqr_photons': float(iqr),
        'pct_range_photons': float(pct_range),
        'max_photons': float(max_val),
    }


def get_local_cfg():
    """Get config with local paths for model checkpoints."""
    return {
        "bright_image_dir": 0.21,
        "dark_image_path": 101,
        "generate_plots": False,
        "num_channels": 4,
        "channel_map": CHANNEL_MAP,
        "roisize": 12,
        "iterations": 30,
        "dev": "cuda",
        "config_paths": LOCAL_YAML_PATHS,
    }


def get_detection_cfg():
    """Get detection configuration."""
    return {
        "uniform_filter1_size": 6,
        "uniform_filter2_size": 12,
        "local_max_filter_size": 9,
        "roisize": 12,
        "min_distance": 10,
    }


def get_fit_cfg():
    """Get fit configuration."""
    return {
        "zslices": 10,
        "bounds_mle_sigma": [[2, 10], [2, 10], [0, 30], [1, 1e9], [1, 1e6], [0.2, 4], [0.2, 4], [0.2, 10]],
        "bounds_mle": [[2, 10], [2, 10], [0, 30], [1, 1e9], [1, 1e6]],
        "iterations": 60,
        "damping_factor": 0.01,
        "initial_sigma": [187/162.5, 191/162.5, 592/500],
    }


def update_yaml_for_local(yaml_path: str, new_model_path: str):
    """Temporarily update the YAML config to use local paths."""
    import yaml

    with open(yaml_path, 'r') as f:
        original_config = yaml.safe_load(f)

    config = dict(original_config)
    config['model_path'] = new_model_path

    if 'loaders' in config and 'test' in config['loaders']:
        local_temp_dir = '/tmp/rna_scope_predict'
        os.makedirs(local_temp_dir, exist_ok=True)
        config['loaders'] = dict(config['loaders'])
        config['loaders']['test'] = dict(config['loaders']['test'])
        config['loaders']['test']['file_paths'] = [f'{local_temp_dir}/predict.h5']

    with open(yaml_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)

    return original_config


def restore_yaml_config(yaml_path: str, original_config: dict):
    """Restore the original YAML config."""
    import yaml
    with open(yaml_path, 'w') as f:
        yaml.dump(original_config, f, default_flow_style=False)


def get_npz_path_from_h5(fov_key: str, h5_path: str) -> str:
    """Get the NPZ file path for a FOV from H5 metadata."""
    import re

    with h5py.File(h5_path, 'r') as f:
        if fov_key not in f:
            return None

        fov = f[fov_key]
        file_path = fov['general_metadata/file_path'][()]

        if isinstance(file_path, bytes):
            file_path = file_path.decode('utf-8')
        elif isinstance(file_path, np.ndarray):
            file_path = file_path.item() if file_path.size == 1 else str(file_path[0])
            if isinstance(file_path, bytes):
                file_path = file_path.decode('utf-8')

    # Check if file exists at stored path
    if Path(file_path).exists():
        return file_path

    local_base = '/media/grunwaldlab/SG Skyhawk AI 24TB/Q111 raw data'

    # Extract slide, region, and FOV from the path
    slide_match = re.search(r'(Slide M\d+ - [AB]\d+)', file_path)
    region_match = re.search(r'Region (\d+)', file_path)
    fov_match = re.search(r'FOV_(\d+)\.npz', file_path)

    if slide_match and region_match and fov_match:
        slide_name = slide_match.group(1)
        region_num = int(region_match.group(1))
        fov_num = int(fov_match.group(1))

        region_dir = f'Region {region_num:03d}'
        region_dir_alt = f'Region {region_num}'
        filename = f'{slide_name}_Region_{region_num}_FOV_{fov_num}.npz'

        dataset_dirs = []
        if 'M1 -' in slide_name:
            dataset_dirs = ['Q111_10slidesno2_june2025_new_illumination/exported']
        elif 'M2 -' in slide_name:
            dataset_dirs = ['Q111_15slidesno1_june2025/exported']
        elif 'M3 -' in slide_name:
            dataset_dirs = ['Q111_10slidesno1_june2025/umcms-scope_grunwald-10slide-part-1_2025-06-26_2015/2025-06-24 11-41_annotated/exported']

        for dataset_dir in dataset_dirs:
            candidate = Path(local_base) / dataset_dir / slide_name / region_dir / filename
            if candidate.exists():
                return str(candidate)

            candidate_alt = Path(local_base) / dataset_dir / slide_name / region_dir_alt / filename
            if candidate_alt.exists():
                return str(candidate_alt)

    return None


def load_npz_image(npz_path: str):
    """Load and decompress NPZ file."""
    with np.load(npz_path, allow_pickle=True) as data:
        compressed_data = data['compressed_data']
        metadata = data['metadata'].item()

    decompressed = blosc.decompress(compressed_data)
    dtype = np.dtype(metadata['dtype'])
    shape = tuple(metadata['shape'])
    arr = np.frombuffer(decompressed, dtype=dtype).reshape(shape)

    return arr, metadata


# ══════════════════════════════════════════════════════════════════════════
# FOV SELECTION
# ══════════════════════════════════════════════════════════════════════════

def select_random_fovs(h5_path: str, n_fovs: int, seed: int = 42) -> list:
    """
    Select random FOVs from different animals/slides.

    Filters:
    - Only Q111 experimental FOVs
    - Only FOVs with local NPZ files available
    - Excludes slides in EXCLUDED_SLIDES
    - Balances across different slides/animals
    """
    random.seed(seed)
    np.random.seed(seed)

    fov_candidates = []

    with h5py.File(h5_path, 'r') as f:
        all_keys = list(f.keys())

        for fov_key in all_keys:
            fov = f[fov_key]

            try:
                # Filter for Q111 experimental only
                probe_set = fov['metadata_sample/Probe-Set'][()]
                if hasattr(probe_set, '__len__') and len(probe_set) > 0:
                    ps = probe_set[0]
                    if isinstance(ps, bytes):
                        ps = ps.decode()
                    if 'Experimental' not in ps:
                        continue
                else:
                    continue

                # Extract slide
                slide = extract_slide_from_fov_key(fov_key)
                if slide is None:
                    continue

                # Skip excluded slides
                if slide in EXCLUDED_SLIDES:
                    continue

                # Check if has cluster data
                if 'green/cluster_intensities' not in fov or 'orange/cluster_intensities' not in fov:
                    continue

                green_n = len(fov['green/cluster_intensities'][:])
                orange_n = len(fov['orange/cluster_intensities'][:])

                if green_n < 5 or orange_n < 5:
                    continue

                fov_candidates.append({
                    'fov_key': fov_key,
                    'slide': slide,
                    'n_clusters': green_n + orange_n,
                })
            except Exception as e:
                continue

    print(f"Found {len(fov_candidates)} candidate FOVs")

    # Group by slide
    slides = {}
    for fov in fov_candidates:
        slide = fov['slide']
        if slide not in slides:
            slides[slide] = []
        slides[slide].append(fov)

    print(f"FOVs distributed across {len(slides)} slides: {sorted(slides.keys())}")

    # Select FOVs trying to balance across slides
    selected = []
    slide_list = list(slides.keys())
    random.shuffle(slide_list)

    # Round-robin selection from each slide
    idx = 0
    while len(selected) < n_fovs * 2:  # Extra buffer for missing NPZ
        slide = slide_list[idx % len(slide_list)]
        if slides[slide]:
            fov = slides[slide].pop(random.randrange(len(slides[slide])))
            selected.append(fov['fov_key'])
        idx += 1

        # Break if we've exhausted all slides
        if all(len(v) == 0 for v in slides.values()):
            break

    print(f"Pre-selected {len(selected)} FOVs for processing")
    return selected


# ══════════════════════════════════════════════════════════════════════════
# CLUSTER PROCESSING
# ══════════════════════════════════════════════════════════════════════════

def process_channel_for_clusters(npz_path: str, channel: str, fov_key: str):
    """
    Process a channel and return cluster data.

    Returns:
        Tuple of (clusters_list, label_mask, converted_image), or None on failure
    """
    yaml_path = LOCAL_YAML_PATHS[channel]
    original_config = update_yaml_for_local(yaml_path, LOCAL_MODEL_PATHS[channel])

    try:
        cfg = get_local_cfg()
        color_cfg = CHANNEL_PARAMS[channel]
        detection_cfg = get_detection_cfg()
        fit_cfg = get_fit_cfg()

        # Create RNAscope instance
        RS = RNAscopeclass(cfg)
        RS.compute_gain(images_max=1)
        RS.image_file = npz_path

        # Load image
        ch_index = color_cfg["channel_index"]
        image_data, channel_name = RS.load_image(npz_path, channel_to_load=ch_index)

        if image_data is None:
            return None

        # Generate labels with UNet
        min_size = color_cfg['min_size']
        max_size = color_cfg['max_size']

        label_mask, label_image, label_sizes = RS.generate_label(
            min_size=min_size,
            max_size=max_size,
            mode=channel
        )

        initial_clusters = len(np.unique(label_mask)) - 1

        # Spot detection and fitting
        (mu_fil, smp_fil, final_params, traces, mip_image,
         filtered_coords, z_starts, filt_indices, pfa_values, params_raw) = RS.detect_and_fit(
            detection_cfg, fit_cfg, image_data, color_cfg,
            label_mask=label_mask, batch_size=5000,
            fit_bg_per_slice=True
        )

        # Sigma-fitting pass
        (mu_fil_sig, smp_fil_sig, final_params_sig, traces_sig, mip_image_sig,
         filtered_coords_sig, z_starts_sig, filt_indices_sig, pfa_values_sig, params_raw_sig) = RS.detect_and_fit(
            detection_cfg, fit_cfg, image_data, color_cfg,
            label_mask=label_mask, batch_size=5000,
            fit_bg_per_slice=True, fit_sigma=filt_indices
        )

        # Break filter and label pruning
        break_sigma = color_cfg.get('break_sigma')
        filter_on_break = None

        if break_sigma is not None and label_mask is not None and final_params_sig is not None:
            filter_on_break = (
                (params_raw_sig[:, -3] < break_sigma[0]) &
                (params_raw_sig[:, -2] < break_sigma[1]) &
                (params_raw_sig[:, -1] < break_sigma[2])
            )
            final_filter = (filt_indices) & (filt_indices_sig) & (np.all(pfa_values <= 0.05, axis=1)) & (filter_on_break)

            # Remove labels touching final-filtered spots
            _, pruned_mip, pruned = RS.remove_labels_touching_spots(
                filtered_coords_sig[final_filter, :], label_mask, dilation_radius=0
            )
            label_mask = pruned

        final_clusters = len(np.unique(label_mask)) - 1

        # Convert image to photons
        converted_image = RS.convert_to_photons(image_data)

        # Analyze cluster intensities
        cluster_intensities, com_array, cluster_label_sizes = RS.analyze_label_intensitiesv2(label_mask, converted_image)

        # Get unique labels
        unique_labels = np.unique(label_mask)
        unique_labels = unique_labels[unique_labels > 0]

        # Create mapping
        label_to_idx = {label_id: idx for idx, label_id in enumerate(unique_labels)}
        label_sizes_bincount = np.bincount(label_mask.ravel())

        # Get slide-specific peak intensity from pre-computed CSV (REQUIRED)
        slide = extract_slide_from_fov_key(fov_key)
        if slide is None:
            print(f"    WARNING: Could not extract slide from FOV key")
            return None

        try:
            peak_intensity = get_slide_peak_intensity(slide, channel)
            print(f"    Peak intensity for {slide}/{channel}: {peak_intensity:.2f} photons")
        except ValueError as e:
            print(f"    ERROR: {e}")
            return None

        # Get photon threshold (required - no fallback)
        try:
            photon_threshold = get_photon_threshold(slide, channel)
        except ValueError as e:
            print(f"    ERROR: {e}")
            return None

        # Build cluster data
        clusters = []

        for label_id in unique_labels:
            if label_id not in label_to_idx:
                continue
            idx = label_to_idx[label_id]
            if idx >= len(cluster_intensities):
                continue

            raw_intensity = cluster_intensities[idx]
            mrna_equiv = raw_intensity / peak_intensity

            # Get volume
            if label_id < len(label_sizes_bincount):
                volume_voxels = label_sizes_bincount[label_id]
                volume_um3 = volume_voxels * VOXEL_VOLUME_UM3
            else:
                volume_voxels = 0
                volume_um3 = 0

            # Compute density
            if volume_um3 > 0:
                density = mrna_equiv / volume_um3
            else:
                density = 0

            # Get centroid from COM array
            if idx < len(com_array):
                cz, cy, cx = com_array[idx]
            else:
                cx, cy, cz = 0, 0, 0

            # Check if passes threshold (threshold is always valid, no fallback)
            passes_threshold = raw_intensity > photon_threshold

            # Compute variance statistics for this cluster
            variance_stats = compute_cluster_variance_stats(label_mask, converted_image, label_id)

            clusters.append({
                'fov_key': fov_key,
                'slide': slide,
                'channel': channel,
                'label_id': int(label_id),
                'raw_intensity': float(raw_intensity),
                'mrna_equiv': float(mrna_equiv),
                'volume_voxels': int(volume_voxels),
                'volume_um3': float(volume_um3),
                'density': float(density),
                'cx': float(cx),
                'cy': float(cy),
                'cz': float(cz),
                'peak_intensity': float(peak_intensity),
                'photon_threshold': float(photon_threshold),
                'passes_threshold': passes_threshold,
                **variance_stats,  # Add variance features
            })

        print(f"    {channel}: {len(clusters)} clusters (initial: {initial_clusters}, after pruning: {final_clusters})")

        return clusters, label_mask, converted_image

    except Exception as e:
        print(f"    ERROR processing {channel}: {e}")
        traceback.print_exc()
        return None

    finally:
        restore_yaml_config(yaml_path, original_config)


# ══════════════════════════════════════════════════════════════════════════
# CLUSTER IMAGE SAVING
# ══════════════════════════════════════════════════════════════════════════

def save_cluster_image(image_3d: np.ndarray, label_mask: np.ndarray,
                       cluster: dict, channel: str, output_path: Path,
                       zoom_size: int = 64):
    """Save a zoomed image of a single cluster."""
    # Get cluster centroid
    cx, cy = int(cluster['cx']), int(cluster['cy'])

    # Create MIP
    mip = np.max(image_3d, axis=0)
    label_mip = np.max(label_mask, axis=0)

    h, w = mip.shape

    # Extract zoom region
    x1 = max(0, cx - zoom_size // 2)
    x2 = min(w, cx + zoom_size // 2)
    y1 = max(0, cy - zoom_size // 2)
    y2 = min(h, cy + zoom_size // 2)

    zoom_mip = mip[y1:y2, x1:x2]
    zoom_label = label_mip[y1:y2, x1:x2]

    # Normalize
    vmin = np.percentile(zoom_mip, 1)
    vmax = np.percentile(zoom_mip, 99.5)
    norm = np.clip((zoom_mip.astype(float) - vmin) / (vmax - vmin + 1e-10), 0, 1)

    # Color
    if channel == 'green':
        rgb = np.zeros((*norm.shape, 3))
        rgb[:, :, 1] = norm
    else:  # orange
        rgb = np.zeros((*norm.shape, 3))
        rgb[:, :, 0] = norm
        rgb[:, :, 1] = norm * 0.65

    rgb = (rgb * 255).astype(np.uint8)

    # Draw cluster contour
    label_id = cluster['label_id']
    binary_mask = (zoom_label == label_id).astype(np.uint8)

    if np.any(binary_mask):
        contours = measure.find_contours(binary_mask, 0.5)

        img = Image.fromarray(rgb)
        draw = ImageDraw.Draw(img)

        h_zoom, w_zoom = binary_mask.shape
        edge_margin = 1.0  # Points within this margin of edge are considered "on edge"

        for contour in contours:
            # Filter out points that are on the image edge
            # These create ugly lines along the border
            filtered_contour = []
            for pt in contour:
                on_edge = (
                    pt[0] <= edge_margin or pt[0] >= h_zoom - edge_margin or
                    pt[1] <= edge_margin or pt[1] >= w_zoom - edge_margin
                )
                if not on_edge:
                    filtered_contour.append(pt)

            if len(filtered_contour) < 3:
                continue

            points = [(int(p[1]), int(p[0])) for p in filtered_contour]

            # Check if the filtered contour is closed (first and last point close together)
            first_pt = filtered_contour[0]
            last_pt = filtered_contour[-1]
            distance = np.sqrt((first_pt[0] - last_pt[0])**2 + (first_pt[1] - last_pt[1])**2)

            if distance < 3:
                # Close the contour
                draw.line(points + [points[0]], fill=(255, 255, 255), width=1)
            else:
                # Open contour - don't close it
                draw.line(points, fill=(255, 255, 255), width=1)

        rgb = np.array(img)

    # Save
    output_path.parent.mkdir(parents=True, exist_ok=True)
    Image.fromarray(rgb).save(output_path)


def save_fov_overview(image_4d: np.ndarray, label_masks: dict,
                      channel: str, output_path: Path):
    """Save FOV overview image with cluster contours."""
    ch_idx = CHANNEL_MAP[channel]
    image_3d = image_4d[ch_idx]

    mip = np.max(image_3d, axis=0)
    label_mask = label_masks.get(channel)

    if label_mask is None:
        return

    label_mip = np.max(label_mask, axis=0)

    # Normalize
    vmin = np.percentile(mip, 1)
    vmax = np.percentile(mip, 99.5)
    norm = np.clip((mip.astype(float) - vmin) / (vmax - vmin + 1e-10), 0, 1)

    # Color
    if channel == 'green':
        rgb = np.zeros((*norm.shape, 3))
        rgb[:, :, 1] = norm
    else:
        rgb = np.zeros((*norm.shape, 3))
        rgb[:, :, 0] = norm
        rgb[:, :, 1] = norm * 0.65

    rgb = (rgb * 255).astype(np.uint8)

    # Draw all cluster contours in white
    img = Image.fromarray(rgb)
    draw = ImageDraw.Draw(img)

    unique_labels = np.unique(label_mip)
    unique_labels = unique_labels[unique_labels > 0]

    for label_id in unique_labels:
        binary_mask = (label_mip == label_id).astype(np.uint8)
        contours = measure.find_contours(binary_mask, 0.5)

        for contour in contours:
            points = [(int(p[1]), int(p[0])) for p in contour]
            if len(points) > 2:
                draw.line(points + [points[0]], fill=(255, 255, 255), width=1)

    rgb = np.array(img)

    output_path.parent.mkdir(parents=True, exist_ok=True)
    Image.fromarray(rgb).save(output_path)


# ══════════════════════════════════════════════════════════════════════════
# ANALYSIS AND VISUALIZATION
# ══════════════════════════════════════════════════════════════════════════

def create_density_distribution_plots(df: pd.DataFrame, output_dir: Path):
    """Create density distribution plots."""
    fig, axes = plt.subplots(3, 2, figsize=(12, 14))

    for i, channel in enumerate(['green', 'orange']):
        ch_data = df[df['channel'] == channel]

        if len(ch_data) == 0:
            continue

        color = 'green' if channel == 'green' else 'orange'

        # Row 0: Histogram of densities
        ax = axes[0, i]
        densities = ch_data['density'].values
        densities = densities[densities > 0]  # Remove zeros
        densities = densities[densities < 1000]  # Cap for visualization

        ax.hist(densities, bins=50, alpha=0.7, color=color)
        ax.set_xlabel('Density (mRNA/µm³)')
        ax.set_ylabel('Count')
        ax.set_title(f'{channel.capitalize()} - Density Distribution')

        # Row 1: Density vs volume scatter
        ax = axes[1, i]
        ax.scatter(ch_data['volume_um3'], ch_data['density'],
                   alpha=0.3, s=5, c=color)
        ax.set_xlabel('Volume (µm³)')
        ax.set_ylabel('Density (mRNA/µm³)')
        ax.set_title(f'{channel.capitalize()} - Density vs Volume')
        # Data-driven axis limits with some padding
        ax.set_xlim(0, ch_data['volume_um3'].quantile(0.99) * 1.1)
        ax.set_ylim(0, ch_data['density'].quantile(0.99) * 1.1)

        # Row 2: Volume vs intensity scatter
        ax = axes[2, i]
        ax.scatter(ch_data['volume_um3'], ch_data['raw_intensity'],
                   alpha=0.3, s=5, c=color)
        ax.set_xlabel('Volume (µm³)')
        ax.set_ylabel('Raw Intensity (photons)')
        ax.set_title(f'{channel.capitalize()} - Volume vs Intensity')
        # Data-driven axis limits with some padding
        ax.set_xlim(0, ch_data['volume_um3'].quantile(0.99) * 1.1)
        ax.set_ylim(0, ch_data['raw_intensity'].quantile(0.99) * 1.1)

    plt.tight_layout()
    plt.savefig(output_dir / 'density_distributions.pdf', dpi=150)
    plt.savefig(output_dir / 'density_distributions.png', dpi=150)
    plt.close()


def create_fov_summary_plots(fov_df: pd.DataFrame, output_dir: Path):
    """Create per-FOV summary plots."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    for i, channel in enumerate(['green', 'orange']):
        ch_data = fov_df[fov_df['channel'] == channel].copy()

        if len(ch_data) == 0:
            continue

        # Sort by mean density
        ch_data = ch_data.sort_values('mean_density', ascending=True)

        # Bar plot of mean density per FOV
        ax = axes[0, i]
        x = range(len(ch_data))
        ax.bar(x, ch_data['mean_density'], color='green' if channel == 'green' else 'orange', alpha=0.7)
        ax.set_xlabel('FOV index (sorted by mean density)')
        ax.set_ylabel('Mean density (mRNA/µm³)')
        ax.set_title(f'{channel.capitalize()} - Mean Density per FOV')

        # Scatter: mean density vs total clusters
        ax = axes[1, i]
        ax.scatter(ch_data['n_clusters'], ch_data['mean_density'],
                   c=ch_data['median_density'], cmap='viridis', s=50, alpha=0.7)
        ax.set_xlabel('Number of clusters')
        ax.set_ylabel('Mean density (mRNA/µm³)')
        ax.set_title(f'{channel.capitalize()} - Mean Density vs Cluster Count')
        cbar = plt.colorbar(ax.collections[0], ax=ax)
        cbar.set_label('Median density')

    plt.tight_layout()
    plt.savefig(output_dir / 'fov_summary.pdf', dpi=150)
    plt.savefig(output_dir / 'fov_summary.png', dpi=150)
    plt.close()


# ══════════════════════════════════════════════════════════════════════════
# MAIN PROCESSING
# ══════════════════════════════════════════════════════════════════════════

def main():
    parser = argparse.ArgumentParser(description='Investigate cluster density distributions')
    parser.add_argument('--n_fovs', type=int, default=100, help='Number of FOVs to analyze')
    parser.add_argument('--output_dir', type=str, default='./output', help='Output directory')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--save_images', action='store_true',
                        help='Save ALL cluster images with density-sortable filenames')
    parser.add_argument('--zoom_size', type=int, default=64,
                        help='Size of cluster zoom images in pixels')
    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    h5_path = H5_FILE_PATH_EXPERIMENTAL

    print(f"="*70)
    print("CLUSTER DENSITY INVESTIGATION")
    print(f"="*70)
    print(f"H5 file: {h5_path}")
    print(f"Output: {output_dir}")
    print(f"N FOVs: {args.n_fovs}")
    print(f"Save images: {args.save_images}")
    print(f"="*70)

    # Verify required files exist
    print("\nVerifying prerequisite files...")
    if not PEAK_INTENSITIES_CSV.exists():
        print(f"ERROR: Peak intensities CSV not found at {PEAK_INTENSITIES_CSV}")
        print("Please run fig_aggregate_scaling_v3.py first to generate this file.")
        sys.exit(1)
    else:
        print(f"  Peak intensities CSV: OK")

    if not PHOTON_THRESHOLDS_CSV.exists():
        print(f"ERROR: Photon thresholds CSV not found at {PHOTON_THRESHOLDS_CSV}")
        print("Please run fig_negative_threshold.py first to generate this file.")
        sys.exit(1)
    else:
        print(f"  Photon thresholds CSV: OK")

    # Select FOVs
    print("\nSelecting FOVs...")
    fov_keys = select_random_fovs(h5_path, args.n_fovs, seed=args.seed)

    # Process FOVs
    all_clusters = []
    processed_count = 0
    fov_times = []  # Track time per FOV

    for fov_idx, fov_key in enumerate(fov_keys):
        if processed_count >= args.n_fovs:
            break

        # Get NPZ path
        npz_path = get_npz_path_from_h5(fov_key, h5_path)
        if npz_path is None:
            print(f"[{fov_idx+1}/{len(fov_keys)}] Skipping {fov_key} - NPZ not found")
            continue

        fov_start_time = time.time()
        print(f"\n[{processed_count+1}/{args.n_fovs}] Processing: {fov_key}")

        try:
            # Load image
            image_4d, metadata = load_npz_image(npz_path)

            label_masks = {}
            images_3d = {}

            for channel in ['green', 'orange']:
                result = process_channel_for_clusters(npz_path, channel, fov_key)

                if result is not None:
                    clusters, label_mask, converted_image = result
                    all_clusters.extend(clusters)
                    label_masks[channel] = label_mask
                    images_3d[channel] = converted_image

            processed_count += 1

            # Save cluster images if requested
            if args.save_images and label_masks:
                fov_clusters = [c for c in all_clusters if c['fov_key'] == fov_key]
                images_saved = 0

                for channel in ['green', 'orange']:
                    if channel not in label_masks or channel not in images_3d:
                        continue

                    ch_clusters = [c for c in fov_clusters if c['channel'] == channel]
                    label_mask = label_masks[channel]
                    image_3d = images_3d[channel]

                    # Save each cluster image with density-sortable filename
                    # Format: {density:07.2f}_{channel}_{slide}_{region}_{label_id}.png
                    # Example: 0025.30_green_m1a1_region003_042.png
                    for cluster in ch_clusters:
                        density = cluster['density']
                        slide = cluster['slide']
                        label_id = cluster['label_id']

                        # Extract region from fov_key
                        import re
                        region_match = re.search(r'region(\d+)', fov_key.lower())
                        region = f"r{region_match.group(1)}" if region_match else "r000"

                        # Filename: density (zero-padded for sorting), channel, slide, region, label
                        filename = f"{density:07.2f}_{channel}_{slide}_{region}_{label_id:04d}.png"
                        img_path = output_dir / 'cluster_images' / filename

                        save_cluster_image(image_3d, label_mask, cluster, channel,
                                          img_path, zoom_size=args.zoom_size)
                        images_saved += 1

                if images_saved > 0:
                    print(f"    Saved {images_saved} cluster images")

            # Save CSV incrementally after each FOV (so data is not lost on crash)
            if all_clusters:
                df_incremental = pd.DataFrame(all_clusters)
                df_incremental.to_csv(output_dir / 'cluster_data.csv', index=False)

            # Record timing
            fov_elapsed = time.time() - fov_start_time
            fov_times.append(fov_elapsed)
            avg_time = sum(fov_times) / len(fov_times)
            remaining = args.n_fovs - processed_count
            eta_seconds = remaining * avg_time
            eta_str = f"{int(eta_seconds // 60)}m {int(eta_seconds % 60)}s"
            print(f"    Time: {fov_elapsed:.1f}s (avg: {avg_time:.1f}s, ETA: {eta_str})")

        except Exception as e:
            print(f"  ERROR: {e}")
            traceback.print_exc()
            continue

    # Print timing summary
    print(f"\n{'='*70}")
    print(f"Processed {processed_count} FOVs, collected {len(all_clusters)} clusters")
    if fov_times:
        total_time = sum(fov_times)
        print(f"Total time: {total_time:.1f}s ({total_time/60:.1f}m)")
        print(f"Average per FOV: {sum(fov_times)/len(fov_times):.1f}s")
    print(f"{'='*70}")

    if len(all_clusters) == 0:
        print("No clusters found. Exiting.")
        return

    # Create DataFrame
    df = pd.DataFrame(all_clusters)

    # Save cluster data
    df.to_csv(output_dir / 'cluster_data.csv', index=False)
    print(f"Saved cluster data: {output_dir / 'cluster_data.csv'}")

    # Create FOV summary
    fov_summary = []
    for (fov_key, channel), group in df.groupby(['fov_key', 'channel']):
        fov_summary.append({
            'fov_key': fov_key,
            'slide': group['slide'].iloc[0],
            'channel': channel,
            'n_clusters': len(group),
            'mean_density': group['density'].mean(),
            'median_density': group['density'].median(),
            'min_density': group['density'].min(),
            'max_density': group['density'].max(),
            'std_density': group['density'].std(),
            'mean_volume': group['volume_um3'].mean(),
            'mean_mrna': group['mrna_equiv'].mean(),
        })

    fov_df = pd.DataFrame(fov_summary)
    fov_df.to_csv(output_dir / 'fov_summary.csv', index=False)
    print(f"Saved FOV summary: {output_dir / 'fov_summary.csv'}")

    # Create plots
    print("\nCreating distribution plots...")
    create_density_distribution_plots(df, output_dir)
    create_fov_summary_plots(fov_df, output_dir)

    # Print summary statistics
    print(f"\n{'='*70}")
    print("SUMMARY STATISTICS")
    print(f"{'='*70}")

    for channel in ['green', 'orange']:
        ch_data = df[df['channel'] == channel]
        if len(ch_data) == 0:
            continue

        print(f"\n{channel.upper()}:")
        print(f"  Total clusters: {len(ch_data)}")
        print(f"  Density (mRNA/µm³):")
        print(f"    Mean: {ch_data['density'].mean():.1f}")
        print(f"    Median: {ch_data['density'].median():.1f}")
        print(f"    Min: {ch_data['density'].min():.1f}")
        print(f"    Max: {ch_data['density'].max():.1f}")
        print(f"    Std: {ch_data['density'].std():.1f}")
        print(f"  Volume (µm³):")
        print(f"    Mean: {ch_data['volume_um3'].mean():.2f}")
        print(f"    Median: {ch_data['volume_um3'].median():.2f}")
        print(f"  mRNA equivalent:")
        print(f"    Mean: {ch_data['mrna_equiv'].mean():.1f}")
        print(f"    Median: {ch_data['mrna_equiv'].median():.1f}")

    # Show FOVs with lowest mean density (potential issues)
    print(f"\n{'='*70}")
    print("FOVs WITH LOWEST MEAN DENSITY (bottom 20)")
    print(f"{'='*70}")

    for channel in ['green', 'orange']:
        ch_fov = fov_df[fov_df['channel'] == channel].copy()
        if len(ch_fov) == 0:
            continue

        ch_fov = ch_fov.sort_values('mean_density', ascending=True)
        print(f"\n{channel.upper()}:")
        for _, row in ch_fov.head(20).iterrows():
            print(f"  {row['slide']:6s} | mean={row['mean_density']:6.1f} | median={row['median_density']:6.1f} | n={row['n_clusters']:3d} clusters")

    # Save sample cluster images if requested
    if args.save_images:
        print(f"\n{'='*70}")
        print("Saving sample cluster images...")
        print(f"{'='*70}")

        # We need to reload images for this - for simplicity, just save the problematic FOV overviews
        # which were already done above
        print("  (Cluster images saved during FOV processing)")

    print(f"\nDone! Results saved to: {output_dir}")


if __name__ == '__main__':
    main()
